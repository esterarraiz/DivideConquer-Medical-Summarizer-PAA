# Synapse ðŸ©º - Sistema de Resumo Multimodal MÃ©dico

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o do projeto Synapse, um sistema web construÃ­do com Streamlit para gerar resumos de consultas mÃ©dicas multimodais (texto + imagem). O sistema utiliza duas abordagens:

1.  **Modelo MDCRAPN:** ImplementaÃ§Ã£o baseada no artigo de Manonmani & Malathi (2025), utilizando a estratÃ©gia Dividir e Conquistar (D&C) com mÃ³dulos GFD-BERT-ViT, MACDM e MDCRAPN treinados em PyTorch.
2.  **API Google Gemini:** Utiliza um modelo generativo multimodal do Google (como o Gemini 1.5 Flash/Pro) para fornecer uma anÃ¡lise mais coerente e profissional, usando o texto e a imagem como contexto.

O frontend permite ao usuÃ¡rio inserir uma pergunta/descriÃ§Ã£o textual e fazer upload de uma imagem, recebendo como resultado principal a anÃ¡lise do Gemini e, opcionalmente, a saÃ­da tÃ©cnica do modelo MDCRAPN treinado.

## Funcionalidades âœ¨

* Interface web interativa criada com Streamlit.
* Upload de imagem (sintomas visuais, exames, etc.).
* Entrada de texto (descriÃ§Ã£o de sintomas, perguntas).
* GeraÃ§Ã£o de resumo/anÃ¡lise concisa utilizando a API Google Gemini (multimodal).
* (Opcional) GeraÃ§Ã£o de resumo utilizando o modelo MDCRAPN treinado localmente (demonstraÃ§Ã£o da arquitetura D&C).
* Design customizado com paleta de cores definida.

## Arquitetura (VisÃ£o Geral) ðŸ›ï¸

1.  **Frontend:** AplicaÃ§Ã£o Streamlit (`app.py`) que gerencia a interface do usuÃ¡rio, upload de arquivos e chamadas para as IAs.
2.  **Modelo MDCRAPN (Local):**
    * Definido em `modelo.py`.
    * Composto pelos mÃ³dulos GFD-BERT-ViT (extraÃ§Ã£o de features), MACDM (agregaÃ§Ã£o contextual) e MDCRAPN (geraÃ§Ã£o de texto D&C).
    * Requer arquivos de pesos (`.pth`) treinados (gerados por `train.py` ou `extract_weights.py`).
3.  **Modelo Gemini (API):**
    * Acessado via biblioteca `google-generativeai`.
    * Utiliza um modelo multimodal prÃ©-treinado do Google (e.g., `gemini-2.5-flash`).
    * Requer uma chave de API vÃ¡lida configurada.

## Tecnologias Utilizadas ðŸš€

* Python 3.x
* Streamlit (Frontend)
* PyTorch (Modelo MDCRAPN)
* Hugging Face Transformers (BERT)
* Hugging Face Datasets (Carregamento de dados para treino)
* google-generativeai (API Gemini)
* Pillow (Processamento de imagem)
* NumPy
* Requests, tqdm (Para download de pesos, se implementado)

## ConfiguraÃ§Ã£o e InstalaÃ§Ã£o Local âš™ï¸

1.  **Clonar o RepositÃ³rio:**
    ```bash
    git clone [https://docs.github.com/pt/repositories/creating-and-managing-repositories/quickstart-for-repositories](https://docs.github.com/pt/repositories/creating-and-managing-repositories/quickstart-for-repositories)
    cd [nome-do-repositorio]
    ```

2.  **Criar Ambiente Virtual (Recomendado):**
    ```bash
    python -m venv .venv
    # Windows
    .\.venv\Scripts\activate
    # Linux/macOS
    source .venv/bin/activate
    ```

3.  **Instalar DependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Obter Pesos do Modelo MDCRAPN (.pth):**
    * **OpÃ§Ã£o A (A partir do Checkpoint):**
        * Coloque o arquivo `checkpoint_epoch_X.pth` (ou o checkpoint final do seu treino) nesta pasta.
        * Execute o script para extrair os pesos individuais:
            ```bash
            python extract_weights.py
            ```
        * Isso deve criar os arquivos `gfd_model_weights.pth`, `macdm_model_weights.pth`, e `mdcrapn_model_weights.pth`.
    * **OpÃ§Ã£o B (Download Direto):** Se vocÃª hospedou os arquivos `.pth` extraÃ­dos online (Google Drive, etc.), certifique-se de que o `app.py` tem a lÃ³gica de download configurada com os IDs/links corretos.

5.  **Configurar Chave da API Gemini (OBRIGATÃ“RIO):** ðŸ”‘
    * **NUNCA coloque a chave diretamente no cÃ³digo `.py`!**
    * **MÃ©todo 1 (VariÃ¡vel de Ambiente - Recomendado):** Antes de rodar o Streamlit, defina a variÃ¡vel de ambiente no seu terminal:
        * *Windows (CMD):* `set GEMINI_API_KEY=SUA_CHAVE_API_AQUI`
        * *Windows (PowerShell):* `$env:GEMINI_API_KEY = "SUA_CHAVE_API_AQUI"`
        * *Linux/macOS:* `export GEMINI_API_KEY=SUA_CHAVE_API_AQUI`
    * **MÃ©todo 2 (Arquivo `.env` - Alternativa Local):**
        * Crie um arquivo chamado `.env` na pasta do projeto.
        * Dentro do `.env`, adicione a linha: `GEMINI_API_KEY=SUA_CHAVE_API_AQUI`
        * Instale `python-dotenv`: `pip install python-dotenv`
        * No inÃ­cio do `app.py`, adicione:
            ```python
            from dotenv import load_dotenv
            load_dotenv()
            # ... (o resto do cÃ³digo que usa os.getenv continua igual)
            ```
    * Obtenha sua chave no [Google AI Studio](https://aistudio.google.com/app/api-keys).

6.  **Logo:** Coloque seu arquivo de logo (ex: `logo.png` ou `logo-white.png`) na pasta do projeto, garantindo que o nome corresponda ao usado no `app.py`.

## Executando a AplicaÃ§Ã£o â–¶ï¸

Com o ambiente virtual ativado e as dependÃªncias instaladas, execute:

```bash
streamlit run app.py
```
## ðŸ§  Treinamento do Modelo MDCRAPN (Opcional)

O treinamento do modelo **MDCRAPN** Ã© **computacionalmente intensivo** e **nÃ£o faz parte da execuÃ§Ã£o do `app.py`**.

O script `train.py` (ou o notebook Colab `modelo1.ipynb`) contÃ©m o cÃ³digo completo para:

- Carregar o dataset **ArkaAcharya/MMQSD_ClipSyntel**  
- PrÃ©-processar os dados  
- Treinar os modelos **GFD-BERT-ViT**, **MACDM** e **MDCRAPN**

### ðŸ’¡ RecomendaÃ§Ã£o
Execute o treinamento em um ambiente com **GPU** (ex: Google Colab, Kaggle ou servidor dedicado).

O treinamento gera arquivos de **checkpoint** (ex: `checkpoint_epoch_X.pth`) que contÃªm os pesos do modelo.  
Use o script `extract_weights.py` para preparar esses pesos para o `app.py`.

---

## âœï¸ Autores

- **Ana JÃºlia Campos Vieira**  
- **Dallyla de Moraes Sousa**  
- **Ester Arraiz de Matos**

*(Universidade Federal do Tocantins - UFT)*

---

**S. P. Manonmani** e **S. Malathi**  
> Manonmani, S. P., & Malathi, S. (2026). *Multi-head divide-and-conquer residual-attention mechanism with pointer network for multimodal question summarization in healthcare.*  
> *Information Processing & Management, 63*, 104348.  
> DOI Link
